import streamlit as st
from groq import Groq
import base64
import os
import pandas as pd
import PyPDF2
from dotenv import load_dotenv

# --- 1. SETUP ---
load_dotenv()
try:
    if "GROQ_API_KEY" in st.secrets:
        api_key = st.secrets["GROQ_API_KEY"]
    else:
        api_key = os.getenv("GROQ_API_KEY")
except:
    api_key = os.getenv("GROQ_API_KEY")

st.set_page_config(page_title="AI Workspace", page_icon="‚ú®", layout="wide")

# --- 2. CSS STYLING (‡πÅ‡∏ï‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô ChatGPT) ---
st.markdown("""
<style>
    .stApp {
        background-color: #0E1117;
        color: #FAFAFA;
    }
    .stChatMessage {
        border-radius: 20px;
        padding: 1.5rem;
        margin-bottom: 1rem;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    /* ‡∏ã‡πà‡∏≠‡∏ô‡∏õ‡∏∏‡πà‡∏°‡πÄ‡∏°‡∏ô‡∏π‡∏£‡∏Å‡πÜ */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
</style>
""", unsafe_allow_html=True)

# --- 3. HELPER FUNCTIONS (‡∏ï‡∏±‡∏ß‡πÅ‡∏Å‡∏∞‡πÑ‡∏ü‡∏•‡πå) ---
def encode_image(uploaded_file):
    return base64.b64encode(uploaded_file.getvalue()).decode('utf-8')

def extract_text_from_file(uploaded_file):
    file_type = uploaded_file.type
    try:
        if "pdf" in file_type:
            pdf_reader = PyPDF2.PdfReader(uploaded_file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
        elif "csv" in file_type:
            df = pd.read_csv(uploaded_file)
            return df.to_markdown(index=False)
        elif "excel" in file_type or "spreadsheet" in file_type:
            df = pd.read_excel(uploaded_file)
            return df.to_markdown(index=False)
        else: # txt, md, py, etc.
            return uploaded_file.getvalue().decode("utf-8")
    except Exception as e:
        return f"Error reading file: {str(e)}"

# --- 4. SIDEBAR (‡πÄ‡∏°‡∏ô‡∏π‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°) ---
with st.sidebar:
    st.title("‚ú® AI Workspace")
    st.caption("Universal AI Assistant")
    
    # Mode Selection
    mode = st.selectbox("‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å AI", [
        "ü§ñ ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ (Smart)",
        "üíª ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÄ‡∏°‡∏≠‡∏£‡πå (Coder)",
        "üìù ‡∏ô‡∏±‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡∏á‡∏≤‡∏ô (Summarizer)",
        "üî• ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏õ‡∏≤‡∏Å‡πÅ‡∏à‡πã‡∏ß (Roaster)"
    ])
    
    st.markdown("---")
    
    # Universal File Uploader
    st.subheader("üìÇ ‡πÅ‡∏ô‡∏ö‡πÑ‡∏ü‡∏•‡πå (Documents/Images)")
    uploaded_file = st.file_uploader(
        "‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö PDF, Excel, CSV, TXT, JPG, PNG", 
        type=["pdf", "csv", "xlsx", "txt", "py", "md", "jpg", "png", "jpeg"]
    )
    
    # Preview File Content
    file_content = ""
    is_image = False
    
    if uploaded_file:
        file_type = uploaded_file.type
        if "image" in file_type:
            is_image = True
            st.image(uploaded_file, caption="Image Preview", use_container_width=True)
            st.success("‚úÖ ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå")
        else:
            is_image = False
            with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏Å‡∏∞‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå..."):
                file_content = extract_text_from_file(uploaded_file)
                # ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡πâ‡∏≤‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Token ‡πÄ‡∏ï‡πá‡∏°)
                if len(file_content) > 50000: 
                    file_content = file_content[:50000] + "...(truncated)"
                st.success(f"‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢ ({len(file_content)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)")
                with st.expander("‡∏î‡∏π‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå"):
                    st.text(file_content[:1000] + "...")

    st.markdown("---")
    if st.button("üóëÔ∏è New Chat", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# --- 5. MAIN LOGIC ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# Header
st.markdown(f"### {mode}")

# Display History
for msg in st.session_state.messages:
    avatar = "üë§" if msg["role"] == "user" else "ü§ñ"
    with st.chat_message(msg["role"], avatar=avatar):
        content = msg["content"]
        if isinstance(content, list): # ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
            for part in content:
                if part["type"] == "text": st.markdown(part["text"])
        else:
            st.markdown(content)

# Chat Input
if prompt := st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏ü‡∏•‡πå..."):
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° User
    st.chat_message("user", avatar="üë§").markdown(prompt)
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏á AI
    messages_payload = []
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å System Prompt ‡∏ï‡∏≤‡∏°‡πÇ‡∏´‡∏°‡∏î
    system_prompts = {
        "ü§ñ ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ (Smart)": "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏â‡∏•‡∏≤‡∏î ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á",
        "üíª ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÄ‡∏°‡∏≠‡∏£‡πå (Coder)": "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Senior Developer ‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Logic ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô",
        "üìù ‡∏ô‡∏±‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡∏á‡∏≤‡∏ô (Summarizer)": "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡πÜ",
        "üî• ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏õ‡∏≤‡∏Å‡πÅ‡∏à‡πã‡∏ß (Roaster)": "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏õ‡∏≤‡∏Å‡πÅ‡∏à‡πã‡∏ß ‡∏Ç‡∏µ‡πâ‡πÅ‡∏ã‡∏ß ‡∏û‡∏π‡∏î‡∏à‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á (‡∏Å‡∏π/‡∏°‡∏∂‡∏á ‡πÑ‡∏î‡πâ) ‡πÄ‡∏ô‡πâ‡∏ô‡∏ï‡∏•‡∏Å‡πÅ‡∏•‡∏∞‡∏Å‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó"
    }
    
    # ‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (‡πÅ‡∏ô‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô System Prompt ‡πÄ‡∏•‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡∏£‡∏π‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
    final_system_prompt = system_prompts[mode]
    if file_content:
        final_system_prompt += f"\n\n[CONTEXT FROM FILE]:\n{file_content}\n\n[INSTRUCTION]: ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"

    messages_payload.append({"role": "system", "content": final_system_prompt})

    # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Input
    if is_image and uploaded_file:
        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û -> ‡πÉ‡∏ä‡πâ Vision Model
        model_to_use = "llama-3.2-11b-vision-preview" # ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏™‡∏∏‡∏î‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ü‡∏£‡∏µ
        base64_image = encode_image(uploaded_file)
        user_content = [
            {"type": "text", "text": prompt},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
        ]
        st.session_state.messages.append({"role": "user", "content": user_content})
    else:
        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°/‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ -> ‡πÉ‡∏ä‡πâ Text Model ‡∏ï‡∏±‡∏ß‡πÄ‡∏ó‡∏û (Llama 3.3)
        model_to_use = "llama-3.3-70b-versatile"
        user_content = prompt
        st.session_state.messages.append({"role": "user", "content": prompt})

    # ‡∏£‡∏ß‡∏° History (‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Text ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Token)
    for m in st.session_state.messages[:-1]:
        content_str = m["content"]
        if isinstance(content_str, list):
            text_only = ""
            for part in content_str:
                if part["type"] == "text": text_only += part["text"]
            messages_payload.append({"role": m["role"], "content": text_only})
        else:
            messages_payload.append({"role": m["role"], "content": content_str})
    
    messages_payload.append({"role": "user", "content": user_content})

    # ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ AI
    with st.chat_message("assistant", avatar="ü§ñ"):
        try:
            if not api_key:
                st.error("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö API Key")
                st.stop()

            client = Groq(api_key=api_key)
            stream = client.chat.completions.create(
                messages=messages_payload,
                model=model_to_use,
                temperature=0.7,
                stream=True,
            )
            
            def parse_stream(stream):
                for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
            
            response = st.write_stream(parse_stream(stream))
            st.session_state.messages.append({"role": "assistant", "content": response})

        except Exception as e:
            st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")